import cv2
import torch
import numpy as np
import time
import threading
import queue
import win32com.client
import pythoncom
import json
import os
from ultralytics import YOLO
from follow_up_service import FollowUpSpeechService

# ==========================================
# FollowUpManager: Handles scheduling and cancellation
# ==========================================
class FollowUpManager:
    def __init__(self, pipeline):
        self.pipeline = pipeline
        self.service = FollowUpSpeechService()
        self.pending_timer = None
        self.lock = threading.Lock()
        self.current_context = None # (label, distance)
        
        # LLM Suppression Logic
        self.llm_call_history = {} # {entity_key: last_call_time}
        self.ALLOWED_CLASSES = {'ì‚¬ëŒ', 'ìë™ì°¨', 'ìì „ê±°'} # person, car, bicycle
        self.MAX_LLM_DIST = 4.0
        self.COOL_DOWN_SEC = 8.0 # 5-8 seconds requirement

    def cancel_pending(self):
        with self.lock:
            if self.pending_timer:
                print("[FollowUpMgr] Cancelling pending follow-up.")
                self.pending_timer.cancel()
                self.pending_timer = None
            self.current_context = None

    def schedule_follow_up(self, label, distance, position_desc, entity_key):
        """Schedules a follow-up with strict suppression gating."""
        # Layer 1: Class and Distance Gating
        if label not in self.ALLOWED_CLASSES:
            # print(f"[FollowUpMgr] LLD Suppressed: {label} is not in whitelist.")
            return

        if distance > self.MAX_LLM_DIST:
            # print(f"[FollowUpMgr] LLM Suppressed: distance {distance:.1f}m > {self.MAX_LLM_DIST}m.")
            return

        # Layer 2: Cool-down Gating
        current_time = time.time()
        last_call = self.llm_call_history.get(entity_key, 0)
        if (current_time - last_call) < self.COOL_DOWN_SEC:
            # print(f"[FollowUpMgr] LLM Suppressed: Cool-down active for {entity_key}.")
            return

        # Passed all gates - proceed to cancel pending and schedule new call
        self.cancel_pending()
        
        with self.lock:
            self.current_context = (label, distance)
            # Record call time to enforce cool-down
            self.llm_call_history[entity_key] = current_time
            
            # Reduced delay since immediate warning is removed
            self.pending_timer = threading.Timer(0.2, self._execute_follow_up, args=(label, distance, position_desc))
            self.pending_timer.start()
            print(f"[FollowUpMgr] Gating Passed. Triggering LLM for {label} at {distance:.1f}m")

    def _generate_rule_based_fallback(self, label, distance, position_desc):
        """Deterministic safety fallback when LLM is unavailable."""
        return f"{position_desc} {distance:.1f}ë¯¸í„°ì— {label}ì´ ìˆìœ¼ë‹ˆ ì£¼ì˜í•˜ì„¸ìš”."

    def _execute_follow_up(self, label, distance, position_desc):
        # Verification: Check if the situation is still relevant? 
        # (For this MVP, we rely on the cancellation being called by the loop if object is gone)
        
        # This function runs in a separate thread (Timer thread)
        # It calls the LLM, which is NOT in the detection loop.
        explanation = self.service.generate_explanation(label, distance, position_desc)
        
        # Fallback if LLM fails (explanation is None)
        if explanation is None:
            print("[FollowUpMgr] LLM API failure. Triggering rule-based fallback.")
            explanation = self._generate_rule_based_fallback(label, distance, position_desc)

        if explanation:
            print(f"[FollowUpMgr] Speech Output: {explanation}")
            # Only play if not cancelled during LLM call
            with self.lock:
                if self.current_context == (label, distance):
                    # Play the explanation through the pipeline's TTS worker
                    self.pipeline.speak(explanation, is_follow_up=True)
                else:
                    print("[FollowUpMgr] Context changed during wait/call, discarding result.")

# Whisper ë° PyAudio ì„¤ì •
WHISPER_AVAILABLE = False
PYAUDIO_AVAILABLE = False
try:
    from faster_whisper import WhisperModel
    import pyaudio
    WHISPER_AVAILABLE = True
    PYAUDIO_AVAILABLE = True
except ImportError:
    pass

# ==========================================
# Optimized MVP Test Pipeline: TTS (ìŒì„± ì•ˆë‚´) ë²„ì „
# ==========================================

class MVPTestPipeline:
    def __init__(self):
        print("ìŒì„± ì§€ì› ëª¨ë“œë¡œ ì „í™˜ ì¤‘... ëª¨ë¸ ë¡œë”© ì¤‘...")
        
        # ì„¤ì •ê°’
        self.inference_size = (320, 320)
        self.frame_skip = 3
        self.frame_count = 0
        self.K_DEPTH = 3000.0 
        self.running = False  # ì œì–´ìš© í”Œë˜ê·¸

        # ìŒì„± ì•ˆë‚´ ì„¤ì • (ë³¼ë¥¨ ë° ë®¤íŠ¸)
        self.volume = 100  # 0 ~ 100
        self.is_muted = False

        # TTS í ë° ìŠ¤ë ˆë“œ ì´ˆê¸°í™”
        self.speech_queue = queue.Queue()
        self.tts_thread = threading.Thread(target=self._tts_worker, daemon=True)
        self.tts_thread.start()

        # STT (ìŒì„± ì¸ì‹) ì´ˆê¸°í™”
        self.stt_thread = None
        if WHISPER_AVAILABLE and PYAUDIO_AVAILABLE:
            try:
                # Whisper 'base' ëª¨ë¸ ë¡œë”© (CPU ì‚¬ìš© ì‹œ ìµœì í™”)
                # ë‹¤êµ­ì–´ ëª¨ë¸ì´ë¯€ë¡œ ì–¸ì–´ë¥¼ koë¡œ ê³ ì •í•˜ë©´ ë” ì •í™•í•¨
                print("Whisper 'base' ëª¨ë¸ ë¡œë”© ì¤‘...")
                self.whisper_model = WhisperModel("base", device="cpu", compute_type="int8")
                self.stt_thread = threading.Thread(target=self._stt_worker, daemon=True)
                self.stt_thread.start()
            except Exception as e:
                print(f"âš ï¸ STT ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        else:
            print(f"âš ï¸ ìŒì„± ëª…ë ¹ì´ ë¹„í™œì„±í™”ë˜ì—ˆìŠµë‹ˆë‹¤. (ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¯¸ì„¤ì¹˜)")
        
        # FollowUp Manager ì´ˆê¸°í™”
        self.follow_up_mgr = FollowUpManager(self)

        # ì‹œì‘ ì•Œë¦¼ (ìŠ¤í”¼ì»¤ í™•ì¸ìš©)
        self.speak("ì‹œìŠ¤í…œì„ ì‹œì‘í•©ë‹ˆë‹¤.")

        # ìŒì„± ìƒíƒœ ê´€ë¦¬
        self.announced_objects = {} # {label: last_seen_time}
        self.announce_timeout = 8.0 # 8ì´ˆ ë™ì•ˆ ì•ˆ ë³´ì´ë©´ ì•ˆë‚´ ëª©ë¡ì—ì„œ ì‚­ì œ (ë‹¤ì‹œ ë‚˜íƒ€ë‚˜ë©´ ë§í•¨)

        # ëª¨ë¸ ë¡œë”©
        self.yolo_model = YOLO('yolov8n.pt') 
        self.depth_model_type = "MiDaS_small"
        self.midas = torch.hub.load("intel-isl/MiDaS", self.depth_model_type, trust_repo=True)
        self.device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
        self.midas.to(self.device).eval()
        
        midas_transforms = torch.hub.load("intel-isl/MiDaS", "transforms", trust_repo=True)
        self.transform = midas_transforms.small_transform if self.depth_model_type == "MiDaS_small" else midas_transforms.dpt_transform

        self.last_objects = []
        self.last_depth_map = None
        self.last_depth_viz = None
        
        # ì›¹ ìŠ¤íŠ¸ë¦¬ë°ìš© ë²„í¼
        self.last_web_frame = None
        self.frame_lock = threading.Lock()

        # í•œêµ­ì–´ í´ë˜ìŠ¤ ë§µ
        self.class_names_ko = {
            'person': 'ì‚¬ëŒ', 'bicycle': 'ìì „ê±°', 'car': 'ìë™ì°¨', 'motorcycle': 'ì˜¤í† ë°”ì´',
            'bus': 'ë²„ìŠ¤', 'truck': 'íŠ¸ëŸ­', 'traffic light': 'ì‹ í˜¸ë“±', 'stop sign': 'ì •ì§€ í‘œì§€íŒ',
            'bench': 'ë²¤ì¹˜', 'dog': 'ê°œ', 'cat': 'ê³ ì–‘ì´', 'backpack': 'ë°°ë‚­', 'umbrella': 'ìš°ì‚°',
            'handbag': 'í•¸ë“œë°±', 'tie': 'ë„¥íƒ€ì´', 'suitcase': 'ì—¬í–‰ê°€ë°©', 'sports ball': 'ê³µ',
            'bottle': 'ë³‘', 'wine glass': 'ì™€ì¸ì”', 'cup': 'ì»µ', 'fork': 'í¬í¬', 'knife': 'ì¹¼',
            'spoon': 'ìˆŸê°€ë½', 'bowl': 'ê·¸ë¦‡', 'banana': 'ë°”ë‚˜ë‚˜', 'apple': 'ì‚¬ê³¼', 'sandwich': 'ìƒŒë“œìœ„ì¹˜',
            'orange': 'ì˜¤ë Œì§€', 'broccoli': 'ë¸Œë¡œì½œë¦¬', 'carrot': 'ë‹¹ê·¼', 'hot dog': 'í•«ë„ê·¸', 'pizza': 'í”¼ì',
            'donut': 'ë„ë„›', 'cake': 'ì¼€ì´í¬', 'chair': 'ì˜ì', 'couch': 'ì†ŒíŒŒ', 'potted plant': 'í™”ë¶„',
            'bed': 'ì¹¨ëŒ€', 'dining table': 'ì‹íƒ', 'toilet': 'ë³€ê¸°', 'tv': 'TV', 'laptop': 'ë…¸íŠ¸ë¶',
            'mouse': 'ë§ˆìš°ìŠ¤', 'remote': 'ë¦¬ëª¨ì»¨', 'keyboard': 'í‚¤ë³´ë“œ', 'cell phone': 'í•¸ë“œí°',
            'microwave': 'ì „ìë ˆì¸ì§€', 'oven': 'ì˜¤ë¸', 'í† ìŠ¤í„°': 'í† ìŠ¤í„°', 'sink': 'ì‹±í¬ëŒ€',
            'refrigerator': 'ëƒ‰ì¥ê³ ', 'book': 'ì±…', 'clock': 'ì‹œê³„', 'vase': 'ê½ƒë³‘', 'scissors': 'ê°€ìœ„',
            'teddy bear': 'ê³°ì¸í˜•', 'hair drier': 'í—¤ì–´ë“œë¼ì´ì–´', 'toothbrush': 'ì¹«ì†”'
        }

        # Walking assistance ROI (Center 40%)
        self.roi_x_min = 0.3
        self.roi_x_max = 0.7

        # Spatial Bucketing for entity differentiation
        self.DIST_BIN_SIZE = 1.5   # meters
        self.POS_BIN_SIZE = 0.1    # 10% of frame width

    def _tts_worker(self):
        """ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ SAPI ì—”ì§„ì„ ì´ˆê¸°í™”í•˜ê³  ì•ˆë‚´ë¥¼ ì²˜ë¦¬ (ê°€ì¥ í™•ì‹¤í•œ ìœˆë„ìš° ë°©ì‹)"""
        pythoncom.CoInitialize()
        speaker = win32com.client.Dispatch("SAPI.SpVoice")
        
        while True:
            # íì—ì„œ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜´ (í…ìŠ¤íŠ¸, ê°•ì œì¤‘ì§€ì—¬ë¶€, follow_upì—¬ë¶€)
            item = self.speech_queue.get()
            if item is None: break
            
            text, force_stop, is_follow_up = item
            
            # ë®¤íŠ¸ ìƒíƒœë©´ ë¬´ì‹œ (ë‹¨, ê°•ì œ ì¢…ë£Œ ì•ˆë‚´ëŠ” ì˜ˆì™¸)
            if self.is_muted and not force_stop:
                self.speech_queue.task_done()
                continue

            # ì‹¤ì‹œê°„ ë³¼ë¥¨ ì ìš©
            speaker.Volume = self.volume

            # force_stopì´ Trueì´ë©´ í˜„ì¬ ë§í•˜ê³  ìˆëŠ” ê²ƒê³¼ ë°€ë ¤ìˆëŠ” íë¥¼ ëª¨ë‘ ë¬´ì‹œí•˜ê³  ì¦‰ì‹œ ë§í•¨
            # SAPI Flag: 2 (SVSFPurgeBeforeSpeak)
            flags = 2 if force_stop else 0
            
            print(f"[TTS ë°œí™” ì‹œì‘] {text} (ê°•ì œì¢…ë£Œ: {force_stop}, í›„ì†: {is_follow_up})")
            try:
                speaker.Speak(text, flags)
            except Exception as e:
                print(f"[TTS ì˜¤ë¥˜] {e}")
            print(f"[TTS ë°œí™” ì™„ë£Œ] {text}")
            
            # í›„ì† ì•ˆë‚´ê°€ ì•„ë‹ˆê³ , ê°•ì œ ì¤‘ì§€ê°€ ì•„ë‹ˆë©´ Managerì—ê²Œ ì™„ë£Œ ì‹ í˜¸
            # (ì‹¤ì œë¡œëŠ” speak() í˜¸ì¶œ ì‹œ Managerë¥¼ í˜¸ì¶œí•˜ê²Œ ë³€ê²½í•  ìˆ˜ ìˆìŒ)
            self.speech_queue.task_done()

    def _stt_worker(self):
        """ë§ˆì´í¬ ì†Œë¦¬ë¥¼ ë“£ê³  Whisperë¡œ ì¸ì‹í•˜ëŠ” ìŠ¤ë ˆë“œ (VAD í¬í•¨)"""
        CHUNK = 1024
        FORMAT = pyaudio.paInt16
        CHANNELS = 1
        RATE = 16000
        SILENCE_THRESHOLD = 500  # ìŒì„± ê°ì§€ ì„ê³„ê°’ (í™˜ê²½ì— ë”°ë¼ ì¡°ì ˆ í•„ìš”)
        SILENCE_DURATION = 1.0   # ì¹¨ë¬µ ì‹œê°„ (ì´ˆ)

        p = pyaudio.PyAudio()
        stream = p.open(format=FORMAT, channels=CHANNELS, rate=RATE, input=True, frames_per_buffer=CHUNK)
        stream.start_stream()

        print("ğŸ™ï¸ Whisper STT ì¤€ë¹„ ì™„ë£Œ. ëª…ë ¹ì„ ê¸°ë‹¤ë¦½ë‹ˆë‹¤...")

        audio_buffer = []
        is_speaking = False
        silence_start = None

        while True:
            data = stream.read(CHUNK, exception_on_overflow=False)
            audio_data = np.frombuffer(data, dtype=np.int16)
            amplitude = np.abs(audio_data).mean()

            if amplitude > SILENCE_THRESHOLD:
                if not is_speaking:
                    is_speaking = True
                    print("ğŸ—£ï¸ ë§í•˜ëŠ” ì¤‘...")
                audio_buffer.append(audio_data)
                silence_start = None
            else:
                if is_speaking:
                    if silence_start is None:
                        silence_start = time.time()
                    
                    audio_buffer.append(audio_data)

                    # ì¼ì • ì‹œê°„ ì´ìƒ ì¹¨ë¬µ ì‹œ ì¸ì‹ ì‹œì‘
                    if time.time() - silence_start > SILENCE_DURATION:
                        print("âŒ› ì¸ì‹ ì¤‘...")
                        # ì˜¤ë””ì˜¤ ë°ì´í„°ë¥¼ Whisper í˜•ì‹ìœ¼ë¡œ ë³€í™˜ (float32, 16kHz)
                        full_audio = np.concatenate(audio_buffer).astype(np.float32) / 32768.0
                        
                        segments, info = self.whisper_model.transcribe(full_audio, language="ko", beam_size=5)
                        text = "".join([segment.text for segment in segments]).strip()
                        
                        if text:
                            print(f"ğŸ‘‚ Whisper ê²°ê³¼: {text}")
                            self.handle_command(text)
                        
                        # ë²„í¼ ë° ìƒíƒœ ì´ˆê¸°í™”
                        audio_buffer = []
                        is_speaking = False
                        silence_start = None

    def handle_command(self, text):
        """ìŒì„± ì¸ì‹ì„ í†µí•´ ë“¤ì–´ì˜¨ í…ìŠ¤íŠ¸ë¥¼ ë¶„ì„í•˜ì—¬ ëª…ë ¹ ìˆ˜í–‰"""
        # ëª…ë ¹ì–´ íŒë³„ (ê³µë°± ì œê±° í›„ ë¹„êµ)
        text = text.replace(" ", "")
        
        if "ì¢…ë£Œ" in text:
            self.speak("ì‹œìŠ¤í…œì„ ì¢…ë£Œí•©ë‹ˆë‹¤.", force_stop=True)
            self.running = False
        elif "ë‹¤ì‹œì‹œì‘" in text or "ë‹¤ì‹œì‹¤í–‰" in text:
            self.speak("ì‹œìŠ¤í…œì„ ë‹¤ì‹œ ì‹œì‘í•©ë‹ˆë‹¤.", force_stop=True)
        elif "ë³¼ë¥¨ì˜¬ë ¤" in text:
            self.volume = min(100, self.volume + 20)
            self.speak(f"ë³¼ë¥¨ì„ ì˜¬ë ¸ìŠµë‹ˆë‹¤. í˜„ì¬ ë³¼ë¥¨ {self.volume}")
        elif "ë³¼ë¥¨ë‚´ë ¤" in text:
            self.volume = max(0, self.volume - 20)
            self.speak(f"ë³¼ë¥¨ì„ ë‚´ë ¸ìŠµë‹ˆë‹¤. í˜„ì¬ ë³¼ë¥¨ {self.volume}")
        elif "ì¡°ìš©íˆí•´" in text or "ì •ì§€í•´" in text:
            self.is_muted = True
            self.speak("ìŒì„± ì•ˆë‚´ë¥¼ ì¼ì‹œ ì •ì§€í•©ë‹ˆë‹¤.", force_stop=True)
        elif "ë§í•´ì¤˜" in text or "ë‹¤ì‹œë§í•´" in text:
            self.is_muted = False
            self.speak("ìŒì„± ì•ˆë‚´ë¥¼ ë‹¤ì‹œ ì‹œì‘í•©ë‹ˆë‹¤.")

    def speak(self, text, force_stop=False, is_follow_up=False):
        """ì•ˆë‚´ ë¬¸êµ¬ë¥¼ íì— ì¶”ê°€ (ë¹„ë™ê¸°)"""
        if force_stop:
            # ê¸°ì¡´ íì— ìŒ“ì¸ ëª¨ë“  ë©”ì‹œì§€ ë¬´ì‹œí•˜ë„ë¡ í ë¹„ìš°ê¸° ì‹œë„
            while not self.speech_queue.empty():
                try:
                    self.speech_queue.get_nowait()
                    self.speech_queue.task_done()
                except:
                    break
        self.speech_queue.put((text, force_stop, is_follow_up))

    def stage2_yolo_optimized(self, frame):
        results = self.yolo_model(frame, imgsz=320, verbose=False) 
        objects = []
        for r in results:
            boxes = r.boxes
            for box in boxes:
                b = box.xyxy[0].cpu().numpy().astype(int)
                cls_id = int(box.cls[0])
                model_label = self.yolo_model.names[cls_id]
                ko_label = self.class_names_ko.get(model_label, model_label)
                objects.append({'box': b, 'label': ko_label})
        return objects

    def stage3_depth_optimized(self, frame):
        small_frame = cv2.resize(frame, (256, 256)) 
        img = cv2.cvtColor(small_frame, cv2.COLOR_BGR2RGB)
        input_batch = self.transform(img).to(self.device)

        with torch.no_grad():
            prediction = self.midas(input_batch)
            prediction = torch.nn.functional.interpolate(
                prediction.unsqueeze(1),
                size=frame.shape[:2],
                mode="bicubic",
                align_corners=False,
            ).squeeze()

        depth_map = prediction.cpu().numpy()
        depth_min, depth_max = depth_map.min(), depth_map.max()
        depth_norm = (255 * (depth_map - depth_min) / (depth_max - depth_min + 1e-5)).astype(np.uint8)
        depth_color = cv2.applyColorMap(depth_norm, cv2.COLORMAP_MAGMA)
        
        return depth_map, depth_color

    def raw_to_meters(self, raw_val):
        if raw_val <= 0: return float('inf')
        meters = self.K_DEPTH / (raw_val + 1e-5)
        return meters

    def run(self):
        cap = cv2.VideoCapture(1) # 0ì€ ë‚´ì¥ì¹´ë©”ë¼, 1ì€ ì™¸ì¥ì¹´ë©”ë¼
        if not cap.isOpened():
            print("ì¹´ë©”ë¼ë¥¼ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return

        window_name_main = "MVP Test - Color (YOLO)"
        window_name_depth = "MVP Test - Depth (MiDaS)"
        cv2.namedWindow(window_name_main)
        cv2.namedWindow(window_name_depth)

        print("\n=== ìŒì„± ì•ˆë‚´(TTS)ê°€ ìµœì í™”ëœ MVP íŒŒì´í”„ë¼ì¸ ì‹œì‘ ===")
        
        # ì‹œì‘ ì‹œ ì•ˆë‚´ ìŒì„± ì¶”ê°€ (ì›¹ì—ì„œ ë‹¤ì‹œ ì‹œì‘í•  ë•Œë„ ë‚˜ì˜´)
        self.speak("ë³´ì¡° ì‹œìŠ¤í…œ ì•ˆë‚´ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.", force_stop=True)

        self.running = True
        last_log_time = 0
        log_interval = 6.0 

        while self.running:
            ret, frame = cap.read()
            if not ret: break
            
            self.frame_count += 1
            current_time = time.time()
            
            # --- íŒŒì´í”„ë¼ì¸ ì—°ì‚° ---
            if self.frame_count % self.frame_skip == 1 or self.last_depth_map is None:
                self.last_objects = self.stage2_yolo_optimized(frame)
                self.last_depth_map, self.last_depth_viz = self.stage3_depth_optimized(frame)
            
            display_frame = frame.copy()
            should_log = (current_time - last_log_time) >= log_interval

            # --- ROI í•„í„°ë§ ë° ê°€ì¥ ê°€ê¹Œìš´ ë¬¼ì²´ ì„ íƒ ---
            h, w = frame.shape[:2]
            roi_left = int(w * self.roi_x_min)
            roi_right = int(w * self.roi_x_max)
            
            closest_obj = None
            min_meters = float('inf')

            for obj in self.last_objects:
                b = obj['box']
                cx = (b[0] + b[2]) // 2
                cy = int(b[3] * 0.9)
                
                # ROI ë‚´ë¶€ì— ì¤‘ì‹¬ì´ ìˆëŠ” ê²½ìš°ì—ë§Œ ì²˜ë¦¬
                if roi_left <= cx <= roi_right:
                    h_d, w_d = self.last_depth_map.shape
                    cx_d, cy_d = max(0, min(cx, w_d-1)), max(0, min(cy, h_d-1))
                    
                    raw_val = self.last_depth_map[cy_d, cx_d]
                    meters = self.raw_to_meters(raw_val)
                    
                    # ê°€ì¥ ê°€ê¹Œìš´ ë¬¼ì²´ ê°±ì‹ 
                    if meters < min_meters:
                        min_meters = meters
                        closest_obj = {
                            'label': obj['label'],
                            'box': b,
                            'meters': meters,
                            'cx': cx
                        }

            # --- ì‹œê°í™” ë° ì•ˆë‚´ ---
            # ROI ê°€ì´ë“œ ë¼ì¸ í‘œì‹œ
            cv2.line(display_frame, (roi_left, 0), (roi_left, h), (0, 0, 255), 2)
            cv2.line(display_frame, (roi_right, 0), (roi_right, h), (0, 0, 255), 2)

            current_entities = set() # (label, dist_bin, pos_bin)
            if closest_obj and min_meters < 10.0:
                b = closest_obj['box']
                label_name = closest_obj['label']
                meters = closest_obj['meters']
                
                # Generate Spatial Composite Key
                dist_bin = int(meters / self.DIST_BIN_SIZE)
                pos_bin = int((closest_obj['cx'] / w) / self.POS_BIN_SIZE)
                entity_key = (label_name, dist_bin, pos_bin)
                
                current_entities.add(entity_key)

                # ì‹œê°í™” (ì„ íƒëœ ë¬¼ì²´ë§Œ ê°•ì¡°)
                cv2.rectangle(display_frame, (b[0], b[1]), (b[2], b[3]), (0, 0, 255), 3)
                cv2.putText(display_frame, f"TARGET: {label_name} {meters:.1f}m", (b[0], b[1]-10), 
                            cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 255), 2)

                # --- ìŒì„± ì•ˆë‚´ ë¡œì§ ---
                if entity_key not in self.announced_objects:
                    # Mark as announced immediately to prevent duplicate triggers
                    self.announced_objects[entity_key] = current_time
                    
                    # Determine position description
                    pos_desc = "ì •ë©´"
                    if closest_obj['cx'] < roi_left + (roi_right - roi_left) * 0.3:
                        pos_desc = "ì•½ê°„ ì™¼ìª½"
                    elif closest_obj['cx'] > roi_left + (roi_right - roi_left) * 0.7:
                        pos_desc = "ì•½ê°„ ì˜¤ë¥¸ìª½"
                    
                    # Trigger natural warning (LLM-based) with strict gating
                    self.follow_up_mgr.schedule_follow_up(label_name, meters, pos_desc, entity_key)

                if should_log:
                    print(f"[ë³´í–‰ ë³´ì¡°] ì¥ì• ë¬¼ ê°ì§€: {label_name} | ê°œì²´ í‚¤: {entity_key} | ê±°ë¦¬: {meters:.1f}m")

            # ì•ˆë‚´ ìƒíƒœ ì—…ë°ì´íŠ¸ (ì˜¤ë«ë™ì•ˆ ì•ˆ ë³´ì¸ ì‚¬ë¬¼ì€ ëª©ë¡ì—ì„œ ì œê±°)
            for entity_key in list(self.announced_objects.keys()):
                if entity_key not in current_entities:
                    label_name = entity_key[0] # tuple (label, dist, pos)
                    # ê°ì§€ ì˜ì—­ì—ì„œ ì‚¬ë¼ì§ -> ì•ˆë‚´ ëª©ë¡ì—ì„œ ì‚­ì œ
                    if current_time - self.announced_objects[entity_key] > self.announce_timeout:
                        del self.announced_objects[entity_key]
                        # ë§Œì•½ ì‚¬ë¼ì§„ ë¬¼ì²´ì— ëŒ€í•œ í›„ì† ì•ˆë‚´ê°€ ì˜ˆì•½ë˜ì–´ ìˆë‹¤ë©´ ì·¨ì†Œ
                        self.follow_up_mgr.cancel_pending()

            if should_log:
                last_log_time = current_time

            # í™”ë©´ í‘œì‹œ

            # í™”ë©´ í‘œì‹œ
            # ì›¹ ìŠ¤íŠ¸ë¦¬ë°ìš©ìœ¼ë¡œ í˜„ì¬ í”„ë ˆì„ ì €ì¥
            with self.frame_lock:
                self.last_web_frame = display_frame.copy()

            cv2.imshow(window_name_main, display_frame)
            if self.last_depth_viz is not None:
                cv2.imshow(window_name_depth, self.last_depth_viz)
            
            # ì¢…ë£Œ ë¡œì§
            key = cv2.waitKey(1) & 0xFF
            if key == ord('q') or key == 27: # Që‚˜ ESC
                break
            
            # ì°½ì´ ë‹«í˜”ëŠ”ì§€ í™•ì¸
            if cv2.getWindowProperty(window_name_main, cv2.WND_PROP_VISIBLE) < 1:
                break

        cap.release()
        cv2.destroyAllWindows()

if __name__ == "__main__":
    pipeline = MVPTestPipeline()
    pipeline.run()
